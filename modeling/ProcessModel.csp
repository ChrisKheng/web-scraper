#import "PAT.Lib.Lists";

#define BUFFER_SIZE 2;
#define NUM_BUFFERS 2;
#define NUM_CRAWLERS 4;

// Model the pool of urls in the internet
// Each integer in the array refers to the next index of the array that should be visited by the crawler in its next iteration
var urls = [18, 19, 20, 18, 21, 22, 21, 23, 24, 24, 25, 26, 27, 28, 28, 29, 26, 30, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1];

// queues: queue of each crawler
var<Lists> queues = new Lists(NUM_CRAWLERS);
var<Lists> buffers = new Lists(NUM_BUFFERS);

// Initialise each queue with some seed urls
// a, b, c ... are just placeholder because of syntax
var a = queues.Append(0, [0]);
var b = queues.Append(1, [3]);
var c = queues.Append(2, [6]);
var d = queues.Append(3, [9]);
//var e = queues.Append(4, [12]);
//var f = queues.Append(5, [15]);

// crawlerSem & builderSem: synchronise the access of buffer between crawler and builder who share the same buffer
// bufferMutex : synchronise the access of buffer between crawlers who share the same buffer
var crawlerSem = [BUFFER_SIZE(NUM_BUFFERS)]; 
var builderSem = [0(NUM_BUFFERS)]; 
var bufferMutex = [1(NUM_BUFFERS)];

var searchURLs = [-1(NUM_CRAWLERS)];
var newURLs = [-1(NUM_CRAWLERS)];

var crawlersJoined = 0;
var buildersJoined = 0;

// i/2 because we want to map crawler 0, 1 to crawlerSem 0 and so on (i.e. 0, 1 -> 0; 2, 3 -> 1; 4, 5 -> 2)
// Same goes to bufferMutex
Crawler(i) = [!queues.isListEmpty(i)]dequeue.i{searchURLs[i] = queues.PollFromList(i)} -> Crawl(i) [] [queues.isListEmpty(i)]CrawlerJoin(i);

Crawl(i) = crawl.i{newURLs[i] = urls[searchURLs[i]]} -> CheckURL(i);

CheckURL(i) = [newURLs[i] != -1]AddToQueue(i) [] [newURLs[i] == -1]BeforeAddToBuffer(i);

AddToQueue(i) = addToQueue.i{queues.AddToList(i, newURLs[i])} -> BeforeAddToBuffer(i);

BeforeAddToBuffer(i) = [crawlerSem[i/2] > 0]grabCrawlerSem.i{crawlerSem[i/2] = crawlerSem[i/2] - 1} -> AddToBuffer(i);

AddToBuffer(i) = [bufferMutex[i/2] > 0]grabBufferMutex.i{bufferMutex[i/2] = bufferMutex[i/2] - 1} -> addToBuffer.i{buffers.AddToList(i/2, searchURLs[i])} -> releaseBufferMutex.i{bufferMutex[i/2] = bufferMutex[i/2] + 1}
	-> AfterAddToBuffer(i);

AfterAddToBuffer(i) = releaseBuilderSem.i{builderSem[i/2] = builderSem[i/2] + 1} -> Crawler(i);

CrawlerJoin(i) = updateCrawlerJoined.i{crawlersJoined = crawlersJoined + 1} -> Skip;

//Terminate() = [joined == NUM_CRAWLERS]programTerminates -> Skip;

// Simulate 2 crawlers sharing the same buffer
Crawlers(i) = |||n:{0..1} @ Crawler(n + i*2);

Transfer(i) = [builderSem[i] == BUFFER_SIZE]grabBuilderSem.i{builderSem[i] = 0} -> removeAllFromBuffer -> releaseCralwerSem.i{crawlerSem[i] = BUFFER_SIZE} -> Transfer(i);

Builder(i) = Transfer(i) interrupt AllCrawlersJoined(i);

AllCrawlersJoined(i) = [crawlersJoined == NUM_CRAWLERS]updateBuilderJoined.i{buildersJoined = buildersJoined + 1} -> Skip;


// Simulate the interaction between 2 crawlers and 1 builder on 1 buffer
CrawlersBuilder(i) = Crawlers(i) || Builder(i);

// Simulate three concurrent occurences of CrawlersBuilder()
CrawlersBuilder3() = |||i:{0..(NUM_BUFFERS - 1)} @ CrawlersBuilder(i);

//System() = CrawlersBuilder(3) ||| Terminate();

#define allCrawlersJoined {crawlersJoined == NUM_CRAWLERS};
#define allBuildersJoined {buildersJoined == NUM_BUFFERS};
// #define bufferIsFull {buildersJoined == NUM_BUFFERS && buffers.Count(0) > 0};

#assert CrawlersBuilder3() deadlockfree;
#assert CrawlersBuilder3() reaches allCrawlersJoined;
#assert CrawlersBuilder3() reaches allBuildersJoined;
// #assert CrawlersBuilder3() reaches bufferIsFull;
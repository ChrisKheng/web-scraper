#import "PAT.Lib.Lists";
ï»¿#import "PAT.Lib.IndexURLTree";

#define BUFFER_SIZE 2;
#define NUM_BUFFERS 3;
#define NUM_CRAWLERS 6;

// Model the pool of urls in the internet
// Each integer in the array refers to the next index of the array that should be visited by the crawler in its next iteration
var urls = [18, 19, 20, 18, 21, 22, 21, 23, 24, 24, 25, 26, 27, 28, 28, 29, 26, 30, 32, 31, 33, 35, 35, 34, 36, 37, 37, 38, 38, 40, 
	39, 41, 42, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9];
	
var result = [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 31, 33, 35, 34, 36, 37, 38, 40, 
	39, 41, 42, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9];

// queues for each crawler, and buffers containing urls to be written to tree
var<Lists> queues = new Lists(NUM_CRAWLERS);
var<Lists> buffers = new Lists(NUM_BUFFERS);

// the index URL tree containing all urls
var tree = new IndexURLTree();

var urlPolled = [-1(NUM_BUFFERS)];
var fileExist = [-1(NUM_BUFFERS)];

// Initialise each queue with some seed urls
// a, b, c ... are just placeholder because of syntax
var a = queues.Append(0, [0, 1, 2]);
var b = queues.Append(1, [3, 4, 5]);
var c = queues.Append(2, [6, 7, 8]);
var d = queues.Append(3, [9, 10, 11]);
var e = queues.Append(4, [12, 13, 14]);
var f = queues.Append(5, [15, 16, 17]);


// crawlerSem & builderSem: synchronise the access of buffer between crawler and builder who share the same buffer
// bufferMutex : synchronise the access of buffer between crawlers who share the same buffer
var crawlerSem = [1(NUM_BUFFERS)]; 
var builderSem = [1(NUM_BUFFERS)]; 
var bufferMutex = [1(NUM_BUFFERS)];
var crawlerURLs = [0(NUM_CRAWLERS)];


// i/2 because we want to map crawler 0, 1 to crawlerSem 0 and so on (i.e. 0, 1 -> 0; 2, 3 -> 1; 4, 5 -> 2)
// Same goes to bufferMutex
Crawler(i) = [crawlerSem[i/2] == 1]grabCrawlerSem.i{crawlerSem[i/2] = -1} -> GetURL(i);
			
GetURL(i) = [!queues.isListEmpty(i)]getUrl.i{crawlerURLs[i] = queues.PollFromList(i)} -> CrawlURL(i) [] [queues.isListEmpty(i)] queueIsEmpty.i -> releaseCrawlerSem.i{crawlerSem[i/2] = 1} -> Skip; //Crawler(i);

CrawlURL(i) = [crawlerURLs[i] > 10] AddToQueue(i) []  [crawlerURLs[i] <= 10]endURL -> releaseCrawlerSem.i{crawlerSem[i/2] = 1} -> AddToBuffer(i);

AddToQueue(i) = addToQueue{queues.AddToList(i, urls[crawlerURLs[i]])} -> releaseCrawlerSem{crawlerSem[i/2] = 1} -> AddToBuffer(i);

AddToBuffer(i) = [bufferMutex[i/2] == 1]grabBufferMutex{bufferMutex[i/2] = -1}
				 -> addToBuffer{buffers.AddToList(i/2, crawlerURLs[i])}
				 -> releaseBufferMutex{bufferMutex[i/2] = 1}
				 -> Crawler(i);

builder(i) = [builderSem[i] == 1]grabBuilderSem.i{builderSem[i] = -1} -> startAddUrlAndContent(i);
startAddUrlAndContent(i) = [bufferMutex[i] == 1]grabBufferMutex.i{bufferMutex[i] = -1} -> checkIfListEmpty(i);
checkIfListEmpty(i) = [!buffers.isListEmpty(i)] listNotEmpty.i -> getUrlFromBuffer(i) [] [buffers.isListEmpty(i)] listEmpty.i -> releaseBufferMutex.i{bufferMutex[i] = 1} -> releaseBuilderSem.i{builderSem[i] = 1} -> Skip; //builder(i);
getUrlFromBuffer(i) = getUrl.i{urlPolled[i] = buffers.PollFromList(i)}  -> releaseBufferMutex{bufferMutex[i] = 1} -> checkIfExistInTree(i);
checkIfExistInTree(i) = [tree.FileExists(urlPolled[i])==false] makeDirectory.i -> createNewFileAtomic(i) [] [tree.FileExists(urlPolled[i])==true] afterAddUrl(i);
createNewFileAtomic(i) = [tree.FileExists(urlPolled[i])==false] createFile.i.urlPolled[i]{tree.CreateFile(urlPolled[i])} -> writeDataToFile(i) [] [tree.FileExists(urlPolled[i])==true] afterAddUrl(i); // If createNewFile is not atomic, will have duplicates. 
// createNewFileNonAtomic(i, url) = nothingForNow; // If createNewFile is not atomic, will have duplicates. 
writeDataToFile(i) = writeData.i.urlPolled[i]{tree.WriteData(urlPolled[i])} -> afterAddUrl(i);
afterAddUrl(i) = restart.i -> releaseBuilderSem{builderSem[i] = 1} -> builder(i);

// Simulate 2 crawlers sharing the same buffer
Crawlers(i) = |||n:{0..1} @ Crawler(n + i*2);

// Simulate the interaction between 2 crawlers and 1 builder on 1 buffer
CrawlersBuilder(i) = Crawlers(i) || builder(i);

// Simulate three concurrent occurences of CrawlersBuilder()
CrawlersBuilder3() = |||i:{0..2} @ CrawlersBuilder(i);

#define noDuplicateFileCreation (true == !tree.FileListDuplicateExists());
#define noDuplicateDataWrite (true == !tree.DataListDuplicateExists());
#define correct (true == tree.CheckCorrectness(result));

#assert CrawlersBuilder3() deadlockfree;
#assert CrawlersBuilder3 |= [] noDuplicateFileCreation;
#assert CrawlersBuilder3 |= [] noDuplicateDataWrite;
#assert CrawlersBuilder3 |=<> correct;

#import "PAT.Lib.IndexURLTree";
#import "PAT.Lib.Lists";

#define BUFFER_SIZE 2;
#define NUM_BUFFERS 2;
#define NUM_CRAWLERS 4;
#define NUM_SEEDS 4;
#define NUM_URLS 6;

// Model the pool of urls in the internet
// Each integer in the array refers to the next index of the array that should be visited by the crawler in its next iteration
var urls = [5, 4, 4, 5, -1, -1];

// queues: queue of each crawler
var tree = new IndexURLTree();
var<Lists> queues = new Lists(NUM_CRAWLERS);
var<Lists> buffers = new Lists(NUM_BUFFERS);

// Initialise each queue with some seed urls
// a, b, c ... are just placeholder because of syntax
var a = queues.Append(0, [0]);
var b = queues.Append(1, [1]);
var c = queues.Append(2, [2]);
var d = queues.Append(3, [3]);


// crawlerSem & builderSem: synchronise the access of buffer between crawler and builder who share the same buffer
// bufferMutex : synchronise the access of buffer between crawlers who share the same buffer
var crawlerSem = [BUFFER_SIZE(NUM_BUFFERS)]; 
var builderSem = [0(NUM_BUFFERS)]; 
var bufferMutex = [1(NUM_BUFFERS)];

var searchURLs = [-1(NUM_CRAWLERS)];
var newURLs = [-1(NUM_CRAWLERS)];

var crawlersJoined = 0;
var buildersJoined = 0;

var fileExist_crawlers = [false(NUM_CRAWLERS)];
var fileExist_builders = [false(NUM_BUFFERS)];
var isNewUrlInQueue = [false(NUM_CRAWLERS)]; 

// To test whether a new URL will be added to crawler's queue if tree alr has the new URL
//var test = tree.CreateFile(4);
//var test1 = tree.CreateFile(5);


// --------------------------------------------------------------------- Crawlers ------------------------------------------------------------------------------------
// i/2 because we want to map crawler 0, 1 to crawlerSem 0 and so on (i.e. 0, 1 -> 0; 2, 3 -> 1; 4, 5 -> 2)
// Same goes to bufferMutex
Crawler(i) = [!queues.isListEmpty(i)]dequeue.i{searchURLs[i] = queues.PollFromList(i)} -> Crawl(i) [] [queues.isListEmpty(i)]CrawlerJoin(i);

Crawl(i) = crawl.i{newURLs[i] = urls[searchURLs[i]]} -> CheckURL(i);

// Check whether the new URL is already in the tree or queue or is a terminating URL (i.e. cannot reach any new URL anymore)
// Need to assign tree.FileExists(i) to a variable first because of syntax (since tree.FileExists(i) is returning bool instead of i)
CheckURL(i) = checkTree.i{var x = tree.FileExists(newURLs[i]); fileExist_crawlers[i] = x} -> checkQueue.i{var y = queues.Contains(i, newURLs[i]); isNewUrlInQueue[i] = y}
	-> ([newURLs[i] != -1 && fileExist_crawlers[i] == false && isNewUrlInQueue[i] == false]AddToQueue(i)
		[] [newURLs[i] == -1 || fileExist_crawlers[i] == true || isNewUrlInQueue[i] == true]BeforeAddToBuffer(i));

AddToQueue(i) = addToQueue.i{queues.AddToList(i, newURLs[i])} -> BeforeAddToBuffer(i);

BeforeAddToBuffer(i) = [crawlerSem[i/2] > 0]grabCrawlerSem.i{crawlerSem[i/2] = crawlerSem[i/2] - 1} -> GrabBufferMutex(i);

GrabBufferMutex(i) = [bufferMutex[i/2] > 0]grabBufferMutex.i{bufferMutex[i/2] = bufferMutex[i/2] - 1} -> CheckBufferDuplicate(i);

CheckBufferDuplicate(i) = [!buffers.Contains(i/2, searchURLs[i]) && searchURLs[i] >= NUM_SEEDS]AddToBuffer(i) [] [buffers.Contains(i/2, searchURLs[i]) || searchURLs[i] < NUM_SEEDS]ReleaseBufferMutex(i);

AddToBuffer(i) = addToBuffer.i{buffers.AddToList(i/2, searchURLs[i])} -> ReleaseBufferMutex(i);

ReleaseBufferMutex(i) = releaseBufferMutex.i{bufferMutex[i/2] = bufferMutex[i/2] + 1} -> AfterAddToBuffer(i);

AfterAddToBuffer(i) = releaseBuilderSem.i{builderSem[i/2] = builderSem[i/2] + 1} -> Crawler(i);

CrawlerJoin(i) = updateCrawlerJoined.i{crawlersJoined = crawlersJoined + 1} -> Skip;

// Simulate 2 crawlers sharing the same buffer
Crawlers(i) = |||n:{0..1} @ Crawler(n + i*2);


// --------------------------------------------------------------------- Builders ------------------------------------------------------------------------------------

Transfer(i) = [builderSem[i] == BUFFER_SIZE]grabBuilderSem.i{builderSem[i] = 0} -> RemoveFromBuffer(i);

RemoveFromBuffer(i) = [!buffers.isListEmpty(i)]removeFromBuffer.i{buffers.PollFromList(i)} -> RemoveFromBuffer(i)
	[] [buffers.isListEmpty(i)]AfterTransfer(i);

AfterTransfer(i) = releaseCralwerSem.i{crawlerSem[i] = BUFFER_SIZE} -> Transfer(i);

Builder(i) = Transfer(i) interrupt AllCrawlersJoined(i);

AllCrawlersJoined(i) = [crawlersJoined == NUM_CRAWLERS]BuilderJoin(i);

BuilderJoin(i) = updateBuilderJoined.i{buildersJoined = buildersJoined + 1} -> Skip;


// Simulate the interaction between 2 crawlers and 1 builder on 1 buffer
CrawlersBuilder(i) = Crawlers(i) || Builder(i);

// Simulate three concurrent occurences of CrawlersBuilder()
CrawlersBuilder3() = |||i:{0..(NUM_BUFFERS - 1)} @ CrawlersBuilder(i);

//System() = CrawlersBuilder(3) ||| Terminate();

#define allCrawlersJoined {crawlersJoined == NUM_CRAWLERS};
#define allBuildersJoined {buildersJoined == NUM_BUFFERS};
#define containsDuplicates {buffers.ListsContainsDuplicates() == true};
#define bufferSize {buffers.Count(0) == 2};
// #define containsURL {buffers.Contains(0, 4)};
// #define bufferIsFull {buildersJoined == NUM_BUFFERS && buffers.Count(0) > 0};

#assert CrawlersBuilder3() deadlockfree;
#assert CrawlersBuilder3() reaches allCrawlersJoined;
#assert CrawlersBuilder3() reaches allBuildersJoined;
#assert CrawlersBuilder3() reaches containsDuplicates;
#assert CrawlersBuilder3() reaches bufferSize;
// #assert CrawlersBuilder3() reaches containsURL;
// #assert CrawlersBuilder3() reaches bufferIsFull;
/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package web.scraper;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.LinkedList;
import java.util.List;
import java.util.TreeSet;
import java.util.stream.*;
import java.util.logging.FileHandler;
import java.util.logging.Level;
import java.util.logging.Logger;
import java.util.logging.SimpleFormatter;

public class App {
    // TreeSet and LinkedList is NOT thread safe!!!
    // Visit
    // https://riptutorial.com/java/example/30472/treemap-and-treeset-thread-safety
    // for how to ensure thread safety using TreeSet.

    // We can try ConcurrentLinkedQueue for buffers instead
    private Logger logger;
    private IndexURLTree tree;
    private List<List<Pair<String, String>>> buffers;

    public App() {
        this.logger = Logger.getLogger("App");
        this.tree = new IndexURLTree();
        this.buffers = new LinkedList<>();
        IntStream.range(0, 3).forEach(x -> buffers.add(new LinkedList<>()));
    }

    public void run() {
        logger.info("Starting........ =D");
        initialise();

        List<String> seeds = getURLSeeds();
        List<List<String>> queues = splitList(seeds, 6);

        Crawler crawler1 = new Crawler(queues.get(0), tree, this.buffers.get(0));
        Crawler crawler2 = new Crawler(queues.get(1), tree, this.buffers.get(0));
        Crawler crawler3 = new Crawler(queues.get(2), tree, this.buffers.get(1));
        Crawler crawler4 = new Crawler(queues.get(3), tree, this.buffers.get(1));
        Crawler crawler5 = new Crawler(queues.get(4), tree, this.buffers.get(2));
        Crawler crawler6 = new Crawler(queues.get(5), tree, this.buffers.get(2));

        IndexBuilder builder1 = new IndexBuilder(tree, this.buffers.get(0));
        IndexBuilder builder2 = new IndexBuilder(tree, this.buffers.get(1));
        IndexBuilder builder3 = new IndexBuilder(tree, this.buffers.get(2));

        crawler1.start();
        crawler2.start();
        crawler3.start();
        crawler4.start();
        crawler5.start();
        crawler6.start();

        // Commented out ib thread start() so that the program will terminate after all crawler threads have
        // returned.
        // If the ib thread is still running, the program will not terminate.
        builder1.start();
        builder2.start();
        builder3.start();

        // what is this for?
        Thread stats = new StatsWriter(tree);
        stats.start();
        
        try {
            crawler1.join();
            logger.info(String.format("crawler %d joined...............................", crawler1.getId()));

            crawler2.join();
            logger.info(String.format("crawler %d joined...............................", crawler2.getId()));

            crawler3.join();
            logger.info(String.format("crawler %d joined...............................", crawler3.getId()));

            crawler4.join();
            logger.info(String.format("crawler %d joined...............................", crawler4.getId()));

            crawler5.join();
            logger.info(String.format("crawler %d joined...............................", crawler5.getId()));

            crawler6.join();
            logger.info(String.format("crawler %d joined...............................", crawler6.getId()));
        } catch (InterruptedException e) {
            logger.severe(e.getMessage());
        }
    }
    

    public void initialise() {
        Runtime.getRuntime().addShutdownHook(new Cleaner(this.tree, this.buffers));

        // The following 2 line removes log from the following 2 sources.
        Logger.getLogger("com.gargoylesoftware").setLevel(Level.OFF);
        Logger.getLogger("org.apache.commons.httpclient").setLevel(Level.OFF);

        try {
            // Configure logger to write to external log file
            // TODO: Should this be removed? As the log file can be very large if run for the whole day.
            FileHandler handler = new FileHandler("./log.txt");
            SimpleFormatter formatter = new SimpleFormatter();
            handler.setFormatter(formatter);
            Logger.getLogger("").addHandler(handler);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    // Read urls from seed file.
    public List<String> getURLSeeds() {
        InputStreamReader reader = new InputStreamReader(System.in);
        BufferedReader bufReader = new BufferedReader(reader);
        return bufReader.lines().collect(Collectors.toList());
    }

    // Split the given url list into the specified number of subLists
    // Condition: number of urls in list given >= num of sublists
    public List<List<String>> splitList(List<String> list, int numSubLists) {
        int portionSize = list.size() / numSubLists;

        List<List<String>> result = new LinkedList<>();
        List<String> temp = new LinkedList<>();
        int count = 0;
        int currNumSubLists = 0;

        for (String url : list) {
            temp.add(url);
            count++;

            if (count == portionSize && currNumSubLists < numSubLists - 1) {
                result.add(temp);
                temp = new LinkedList<>();
                currNumSubLists++;
                count = 0;
            }
        }
        result.add(temp);

        return result;
    }

    public static void main(String[] args) {
        new App().run();
    }
}

/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package web.scraper;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.LinkedList;
import java.util.List;
import java.util.TreeSet;
import java.util.logging.Level;
import java.util.stream.Collectors;
import java.util.logging.Logger;

public class App {

    public void run() {
        Logger logger = Logger.getLogger("App");
        // The following 2 line removes log from the following 2 sources.
        //java.util.logging.Logger.getLogger("com.gargoylesoftware").setLevel(Level.OFF);
        //java.util.logging.Logger.getLogger("org.apache.commons.httpclient").setLevel(Level.OFF);
        List<String> seeds = getURLSeeds();

        // TreeSet and LinkedList is NOT thread safe!!!
        // Visit https://riptutorial.com/java/example/30472/treemap-and-treeset-thread-safety
        // for how to ensure thread safety using TreeSet.
        TreeSet<String> tree = new TreeSet<>();
        List<String> buffer1 = new LinkedList<>();
        List<String> buffer2 = new LinkedList<>();

        logger.info("Starting........ =D");
        
        Crawler crawler1 = new Crawler(seeds.subList(0,1), tree, buffer1);
        Crawler crawler2 = new Crawler(seeds.subList(1,2), tree, buffer2);

        IndexBuilder indexBuilder = new IndexBuilder(tree, buffer1);

        Thread t11 = new Thread(crawler1);
        Thread t12 = new Thread(crawler1);
        Thread t21 = new Thread(crawler2);
        Thread t22 = new Thread(crawler2);

        
        t11.start();
        t12.start();
        t21.start();
        t22.start();


        Thread ib1 = new Thread(indexBuilder);
        ib1.start();

        // Spawn and start crawler thread
        // seeds can be split into different portion and give to the individual threads.
        // Crawler crawler = new Crawler(seeds, tree, buffer);
        // crawler.start();

        try {
            t11.join();
            t12.join();
            t21.join();
            t22.join();
        } catch (InterruptedException e) {
            e.printStackTrace();
        }

        writeToDisk(tree);
        logger.info("Done........ =D");
        System.out.println(tree.size());
    }

    // Read urls from seed file.
    public static List<String> getURLSeeds() {
        InputStreamReader reader = new InputStreamReader(System.in);
        BufferedReader bufReader = new BufferedReader(reader);
        return bufReader.lines().collect(Collectors.toList());
    }

    // Write the urls to the disk.
    public static void writeToDisk(TreeSet<String> tree) {
        try {
            File file = new File("./result.txt");
            file.createNewFile();

            FileWriter writer = new FileWriter(file);
            tree.forEach(url -> {
                try {
                    writer.write(url);
                    writer.write("\n");
                } catch (IOException e) {
                    e.printStackTrace();
                }
            });
            writer.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args) {
        new App().run();
    }
}
